{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiheddachraoui/Covid19-ultrasound-Compressed-video-based-Frame-level-Scoring/blob/main/frame_level_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfuqyGdp0cM7"
      },
      "source": [
        "# scoring using Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exCuSrEN0cM7"
      },
      "source": [
        "## Import The Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IU3as4W0cM7",
        "outputId": "f68492a5-9f4f-44be-f0f6-dbd20079bc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# basics\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "import cv2\n",
        "import pickle \n",
        "from PIL import Image\n",
        "import shutil\n",
        "# Keras Libraries\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer, Activation\n",
        "from keras.utils import  load_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.metrics import AUC\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import keras.backend\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from tensorflow.python.framework import ops\n",
        "import inspect\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-txyLjSbeifK"
      },
      "source": [
        "## utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI2-jctMeP0-"
      },
      "outputs": [],
      "source": [
        "# print date and time for given type of representation\n",
        "def date_time(x):\n",
        "    if x==1:\n",
        "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
        "    if x==2:    \n",
        "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
        "    if x==3:  \n",
        "        return 'Date now: %s' % datetime.datetime.now()\n",
        "    if x==4:  \n",
        "        return 'Date today: %s' % datetime.date.today() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eshN2RCXepJg"
      },
      "outputs": [],
      "source": [
        "# reset tensorflow graph tp free up memory and resource allocation \n",
        "def reset_graph(model=None):\n",
        "    if model:\n",
        "        try:\n",
        "            del model\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "   \n",
        "    ops.reset_default_graph()\n",
        "    K.clear_session()\n",
        "    \n",
        "    gc.collect()\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "# reset callbacks \n",
        "def reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
        "    checkpoint = None\n",
        "    reduce_lr = None\n",
        "    early_stopping = None\n",
        "    tensorboard = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMObDy_hqRZ"
      },
      "outputs": [],
      "source": [
        "def create_directory(directory_path, remove=False):\n",
        "    if remove and os.path.exists(directory_path):\n",
        "        try:\n",
        "            shutil.rmtree(directory_path)\n",
        "            os.mkdir(directory_path)\n",
        "        except:\n",
        "            print(\"Could not remove directory : \", directory_path)\n",
        "            return False\n",
        "    else:\n",
        "        try:\n",
        "            os.mkdir(directory_path)\n",
        "        except:\n",
        "            print(\"Could not create directory: \", directory_path)\n",
        "            return False\n",
        "        \n",
        "    return True\n",
        "\n",
        "# Removes directory, if directory exists \n",
        "def remove_directory(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        try:\n",
        "            shutil.rmtree(directory_path)\n",
        "        except:\n",
        "            print(\"Could not remove directory : \", directory_path)\n",
        "            return False\n",
        "        \n",
        "    return True\n",
        "\n",
        "def clear_directory(directory_path):\n",
        "    dirs_files = os.listdir(directory_path)\n",
        "    \n",
        "    for item in dirs_files:\n",
        "#         item_path = os.path.join(directory_path, item)\n",
        "        item_path = directory_path+ item\n",
        "        \n",
        "        try:\n",
        "            if os.path.isfile(item_path):\n",
        "                os.unlink(item_path)\n",
        "            elif os.path.isdir(item_path): \n",
        "                shutil.rmtree(item_path)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            \n",
        "    return True\n",
        "\n",
        "\n",
        "def remove_empty_folders(path, removeRoot=True):\n",
        "    if not os.path.isdir(path):\n",
        "        return\n",
        "    \n",
        "    # remove empty subfolders\n",
        "    files = os.listdir(path)\n",
        "    \n",
        "    if len(files):\n",
        "        for f in files:\n",
        "            fullpath = os.path.join(path, f)\n",
        "            \n",
        "            if os.path.isdir(fullpath):\n",
        "                remove_empty_folders(fullpath)\n",
        "\n",
        "    # if folder empty, delete it\n",
        "    files = os.listdir(path)\n",
        "    \n",
        "    if len(files) == 0 and removeRoot:\n",
        "        print(\"Removing empty folder:\", path)\n",
        "        os.rmdir(path)\n",
        "        \n",
        "        \n",
        "def dir_file_count(directory):\n",
        "    return sum([len(files) for r, d, files in os.walk(directory)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmSqutyd-nCA"
      },
      "outputs": [],
      "source": [
        "def create_charts(cnn, cnn_model):\n",
        "    ## Define train & validation loss\n",
        "    train_loss = cnn_model.history['loss']\n",
        "    val_loss = cnn_model.history['val_loss']\n",
        "    \n",
        "    ## Define train & validation AUC\n",
        "    train_auc_name = list(cnn_model.history.keys())[3]\n",
        "    val_auc_name = list(cnn_model.history.keys())[1]\n",
        "    train_auc = cnn_model.history[train_auc_name]\n",
        "    val_auc = cnn_model.history[val_auc_name]\n",
        "    \n",
        "    ## Define y_pred & y_true\n",
        "    y_true = test_generator.classes\n",
        "    Y_pred = cnn.predict_generator(test_generator, steps = len(test_generator))\n",
        "    y_pred = (Y_pred > 0.5).T[0]\n",
        "    y_pred_prob = Y_pred.T[0]\n",
        "    \n",
        "    ## PLOT ##\n",
        "    fig = plt.figure(figsize=(13, 10))\n",
        "    \n",
        "    ## PLOT 1: TRAIN VS. VALIDATION LOSS \n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title(\"Training vs. Validation Loss\")\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(val_loss, label='validation loss')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "\n",
        "    ## PLOT 2: TRAIN VS. VALIDATION AUC\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title(\"Training vs. Validation AUC Score\")\n",
        "    plt.plot(train_auc, label='training auc')\n",
        "    plt.plot(val_auc, label='validation auc')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "    \n",
        "    ## PLOT 3: CONFUSION MATRIX\n",
        "    plt.subplot(2,2,3)\n",
        "      # Set up the labels for in the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    names = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
        "    counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    ticklabels = ['Normal', 'Pneumonia']\n",
        "\n",
        "      # Create confusion matrix as heatmap\n",
        "    sns.set(font_scale = 1.4)\n",
        "    ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Oranges', xticklabels=ticklabels, yticklabels=ticklabels )\n",
        "    plt.xticks(size=12)\n",
        "    plt.yticks(size=12)\n",
        "    plt.title(\"Confusion Matrix\") #plt.title(\"Confusion Matrix\\n\", fontsize=10)\n",
        "    plt.xlabel(\"Predicted\", size=14)\n",
        "    plt.ylabel(\"Actual\", size=14) \n",
        "    #plt.savefig('cm.png', transparent=True) \n",
        "    \n",
        "    ## PLOT 4: ROC CURVE\n",
        "    plt.subplot(2,2,4)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    plt.title('ROC Curve')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label = \"Random (AUC = 50%)\")\n",
        "    plt.plot(fpr, tpr, label='CNN (AUC = {:.2f}%)'.format(auc*100))\n",
        "    plt.xlabel('False Positive Rate', size=14)\n",
        "    plt.ylabel('True Positive Rate', size=14)\n",
        "    plt.legend(loc='best')\n",
        "    #plt.savefig('roc.png', bbox_inches='tight', pad_inches=1)\n",
        "    \n",
        "    ## END PLOTS\n",
        "    plt.tight_layout();\n",
        "    \n",
        "    ## Summary Statistics\n",
        "    TN, FP, FN, TP = cm.ravel() # cm[0,0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "    accuracy = (TP + TN) / np.sum(cm) # % positive out of all predicted positives\n",
        "    precision = TP / (TP+FP) # % positive out of all predicted positives\n",
        "    recall =  TP / (TP+FN) # % positive out of all supposed to be positives\n",
        "    specificity = TN / (TN+FP) # % negative out of all supposed to be negatives\n",
        "    f1 = 2*precision*recall / (precision + recall)\n",
        "    stats_summary = '[Summary Statistics]\\nAccuracy = {:.2%} | Precision = {:.2%} | Recall = {:.2%} | Specificity = {:.2%} | F1 Score = {:.2%}'.format(accuracy, precision, recall, specificity, f1)\n",
        "    print(stats_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrRD9ORterTQ"
      },
      "source": [
        "## code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044vnlVP0cM8",
        "outputId": "b39fc15a-6439-4d02-c361-9bc2a9eac44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing empty folder: /content/gdrive/MyDrive/Dataset/output/models/2023-01-09 19-54-54\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "input_directory = \"/content/gdrive/MyDrive/Dataset/grayscale_data/\"\n",
        "output_directory = \"/content/gdrive/MyDrive/Dataset/output/\"\n",
        "\n",
        "train_path = input_directory + r\"train\"\n",
        "val_path = input_directory + r\"valid\"\n",
        "test_path = input_directory + r\"test\"\n",
        "\n",
        "figure_directory = \"output/figures\" \n",
        "file_name_pred_batch = figure_directory+r\"/result\"\n",
        "file_name_pred_sample = figure_directory+r\"/sample\"\n",
        "\n",
        "main_model_dir = output_directory + r\"models/\"\n",
        "main_log_dir = output_directory + r\"logs/\"\n",
        "\n",
        "\n",
        "clear_directory(main_log_dir)\n",
        "remove_empty_folders(main_model_dir, False)\n",
        "\n",
        "\n",
        "model_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"/\"\n",
        "log_dir = main_log_dir + time.strftime('%Y-%m-%d %H-%M-%S')\n",
        "\n",
        "create_directory(model_dir, remove=True)\n",
        "create_directory(log_dir, remove=True)\n",
        "\n",
        "model_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9I5nUSRexqB"
      },
      "outputs": [],
      "source": [
        "reset_graph()\n",
        "reset_callbacks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTu9IeSrhOzl"
      },
      "source": [
        "### callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6EluX-fYKo",
        "outputId": "c2f5feb5-182f-4390-dc3f-c6022195aa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settting Callbacks at  Timestamp: 2023-01-09 20:11:05\n",
            "Set Callbacks at  Timestamp: 2023-01-09 20:11:05\n"
          ]
        }
      ],
      "source": [
        "print(\"Settting Callbacks at \", date_time(1))\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_file, \n",
        "    monitor='val_acc', \n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    batch_size=batch_size,\n",
        "    update_freq = 'batch')\n",
        "\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    cooldown=2,\n",
        "    min_lr=0.0000000001,\n",
        "    verbose=1)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
        "# callbacks = [checkpoint, tensorboard]\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "print(\"Set Callbacks at \", date_time(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzhneEFRlzUM"
      },
      "source": [
        "### preprosessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMFmruAz0cM7"
      },
      "outputs": [],
      "source": [
        "# Set a seed value\n",
        "seed_value= 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "#tf.set_random_seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
        "#model.add(Dropout(0.25, seed=seed_value))\n",
        "\n",
        "#6 Configure a new global `tensorflow` session\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUJRKGr70cM8"
      },
      "outputs": [],
      "source": [
        "#hyper_mode = 'grayscale'\n",
        "#imagesize=(688, 1760, 3)\n",
        "#imagesize=(688, 880,6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''mess=[]\n",
        "def convert_image_to_grayscale(image_path, output_folder):\n",
        "    try:\n",
        "      output_filename = os.path.basename(image_path)\n",
        "\n",
        "    # Compute the full path of the output file\n",
        "      output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "    # Check if the output file already exists\n",
        "      if not os.path.exists(output_path):\n",
        "    # Open the image\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('L')\n",
        "        #image = np.expand_dims(image, axis=-1)\n",
        "        # Save the grayscale image\n",
        "        image.save(output_path, 'JPEG')\n",
        "        print(output_path)\n",
        "      \n",
        "    except:\n",
        "      mess.append(image_path)\n",
        "      \n",
        "    # Convert the image to grayscale\n",
        "   \n",
        "\n",
        "def convert_folder_to_grayscale(folder_path, output_folder):\n",
        "    # Get a list of all the files in the folder\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    filenames = os.listdir(folder_path)\n",
        "\n",
        "    # Iterate over the files\n",
        "    for filename in filenames:\n",
        "        # Compute the full path of the file\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Check if the file is an image\n",
        "        if os.path.splitext(file_path)[1].lower() in ['.jpg', '.png']:\n",
        "            # Convert the image to grayscale and save it to the output folder\n",
        "            convert_image_to_grayscale(file_path, output_folder)\n",
        "\n",
        "# Convert all images in the \"input\" directory to grayscale and save them to the \"output\" directory\n",
        "s=['score0','score1','score2','score3']\n",
        "for i in s:\n",
        "  try:\n",
        "    convert_folder_to_grayscale(r'/content/gdrive/MyDrive/Dataset/frames_dataset/valid/{}'.format(i), r'/content/gdrive/MyDrive/Dataset/grayscale_data/valid/{}'.format(i))\n",
        "    #convert_folder_to_grayscale(r'/content/gdrive/MyDrive/Dataset/frames_dataset/test/{}'.format(i), r'/content/gdrive/MyDrive/Dataset/grayscale_data/test/{}'.format(i))\n",
        "    convert_folder_to_grayscale(r'/content/gdrive/MyDrive/Dataset/frames_dataset/train/{}'.format(i), r'/content/gdrive/MyDrive/Dataset/grayscale_data/train/{}'.format(i))\n",
        "  except Exception: \n",
        "\n",
        "    continue\n",
        "print(mess)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "7y-GLuuuCPKF",
        "outputId": "44db26b0-0dca-4bab-a727-8751c5f9cc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mess=[]\\ndef convert_image_to_grayscale(image_path, output_folder):\\n    try:\\n      output_filename = os.path.basename(image_path)\\n\\n    # Compute the full path of the output file\\n      output_path = os.path.join(output_folder, output_filename)\\n\\n    # Check if the output file already exists\\n      if not os.path.exists(output_path):\\n    # Open the image\\n        image = Image.open(image_path)\\n        image = image.convert(\\'L\\')\\n        #image = np.expand_dims(image, axis=-1)\\n        # Save the grayscale image\\n        image.save(output_path, \\'JPEG\\')\\n        print(output_path)\\n      \\n    except:\\n      mess.append(image_path)\\n      \\n    # Convert the image to grayscale\\n   \\n\\ndef convert_folder_to_grayscale(folder_path, output_folder):\\n    # Get a list of all the files in the folder\\n    if not os.path.exists(output_folder):\\n        os.makedirs(output_folder)\\n    filenames = os.listdir(folder_path)\\n\\n    # Iterate over the files\\n    for filename in filenames:\\n        # Compute the full path of the file\\n        file_path = os.path.join(folder_path, filename)\\n\\n        # Check if the file is an image\\n        if os.path.splitext(file_path)[1].lower() in [\\'.jpg\\', \\'.png\\']:\\n            # Convert the image to grayscale and save it to the output folder\\n            convert_image_to_grayscale(file_path, output_folder)\\n\\n# Convert all images in the \"input\" directory to grayscale and save them to the \"output\" directory\\ns=[\\'score0\\',\\'score1\\',\\'score2\\',\\'score3\\']\\nfor i in s:\\n  try:\\n    convert_folder_to_grayscale(r\\'/content/gdrive/MyDrive/Dataset/frames_dataset/valid/{}\\'.format(i), r\\'/content/gdrive/MyDrive/Dataset/grayscale_data/valid/{}\\'.format(i))\\n    #convert_folder_to_grayscale(r\\'/content/gdrive/MyDrive/Dataset/frames_dataset/test/{}\\'.format(i), r\\'/content/gdrive/MyDrive/Dataset/grayscale_data/test/{}\\'.format(i))\\n    convert_folder_to_grayscale(r\\'/content/gdrive/MyDrive/Dataset/frames_dataset/train/{}\\'.format(i), r\\'/content/gdrive/MyDrive/Dataset/grayscale_data/train/{}\\'.format(i))\\n  except Exception: \\n\\n    continue\\nprint(mess)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''for s in ['train','valid']:\n",
        "    \n",
        "    \n",
        "    # Select a random sample of size 100 from the source directory\n",
        "    path = r\"/content/gdrive/MyDrive/Dataset/grayscale_data/{}\".format(s)\n",
        "\n",
        "    # List the files in the folder\n",
        "    for root, dirs, filenames in os.walk(path, topdown=False):\n",
        "    for subfolder in dirs:\n",
        "        subfolder_path = os.path.join(root, subfolder)\n",
        "        filenames = os.listdir(subfolder_path)\n",
        "\n",
        "        # Calculate the number of files to keep\n",
        "        num_files = int(len(filenames) * 0.4)\n",
        "\n",
        "        # Randomly select a subset of the files to keep\n",
        "        filenames_new = random.sample(filenames, num_files)\n",
        "\n",
        "        # Delete the rest of the files\n",
        "        for filename in filenames:\n",
        "            p = os.path.join(subfolder_path, filename)\n",
        "            if filename not in filenames_new:\n",
        "                os.remove(p)\n",
        "                print(p)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "r1n7KhAIKMXk",
        "outputId": "bf251910-b6e2-4dfa-e352-2c82a162e09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for s in [\\'train\\',\\'valid\\']:\\n    \\n    \\n    # Select a random sample of size 100 from the source directory\\n    path = r\"/content/gdrive/MyDrive/Dataset/grayscale_data/{}\".format(s)\\n\\n    # List the files in the folder\\n    for root, dirs, filenames in os.walk(path, topdown=False):\\n    for subfolder in dirs:\\n        subfolder_path = os.path.join(root, subfolder)\\n        filenames = os.listdir(subfolder_path)\\n\\n        # Calculate the number of files to keep\\n        num_files = int(len(filenames) * 0.4)\\n\\n        # Randomly select a subset of the files to keep\\n        filenames_new = random.sample(filenames, num_files)\\n\\n        # Delete the rest of the files\\n        for filename in filenames:\\n            p = os.path.join(subfolder_path, filename)\\n            if filename not in filenames_new:\\n                os.remove(p)\\n                print(p)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SX2kMAcTG1Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEmQB-Lg0cM8",
        "outputId": "6500ccb9-6355-43be-df9e-bbf63457f174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3502 images belonging to 4 classes.\n",
            "Found 1457 images belonging to 4 classes.\n",
            "Found 1073 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0, \n",
        "                                   shear_range = 0.02,\n",
        "                                   zoom_range = 0.02, \n",
        "                                 \n",
        "                                   #preprocessing_function=preprocess_image ,\n",
        "                                   horizontal_flip = False)\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                                              \n",
        "                                                               #preprocessing_function=preprocess_image \n",
        "                                                              ) \n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                                                \n",
        "                                                               #preprocessing_function=preprocess_image\n",
        "                                                               ) \n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory = train_path, \n",
        "                                                    \n",
        "                                                    target_size = (240, 240), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "val_generator = val_datagen.flow_from_directory(directory = val_path, \n",
        "                                                 \n",
        "                                                    target_size = (240, 240), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "test_generator = test_datagen.flow_from_directory(directory = test_path, \n",
        "                                                 \n",
        "                                                    target_size = (240, 240), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "\n",
        "test_generator.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgfidq8346qn",
        "outputId": "ffaf0cf4-74cd-4346-e5d3-8f5d43d9d5cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 240, 240, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "x,y = next(train_generator)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OX8NyzKs5Zlo",
        "outputId": "f76ee64c-3bd1-4ca1-bbe8-8bcbcf3001e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a = train_generator.class_indices\\nclass_names = list(a.keys())  # storing class/breed names in a list\\n \\n \\ndef plot_images(img, labels):\\n    plt.figure(figsize=[15, 10])\\n    for i in range(batch_size):\\n        plt.subplot(5, 5, i+1)\\n        plt.imshow(img[i])\\n        plt.title(class_names[np.argmax(labels[i])])\\n        plt.axis('off')\\n \\nplot_images(x,y)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "'''a = train_generator.class_indices\n",
        "class_names = list(a.keys())  # storing class/breed names in a list\n",
        " \n",
        " \n",
        "def plot_images(img, labels):\n",
        "    plt.figure(figsize=[15, 10])\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(img[i])\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis('off')\n",
        " \n",
        "plot_images(x,y)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgR0YAgrktB1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxNPT3951lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3b7b0a-886b-4b8b-caf6-fcc1d4ed343d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load the InceptionResNetV2 architecture with imagenet weights as base\n",
        "base_model = tf.keras.applications.resnet.ResNet50(\n",
        "                     include_top=False,\n",
        "                     weights='imagenet',\n",
        "                     input_shape=(240, 240,3),\n",
        "                     \n",
        "                    \n",
        "                     )\n",
        " \n",
        "base_model.trainable=False\n",
        "\n",
        "# For freezing the layer we make use of layer.trainable = False\n",
        "# means that its internal state will not change during training.\n",
        "# model's trainable weights will not be updated during fit(),\n",
        "# and also its state updates will not run.\n",
        " \n",
        "cnn = tf.keras.Sequential([\n",
        "        base_model \n",
        "         ])\n",
        "\"\"\"  ,tf.keras.layers.BatchNormalization(renorm=True),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        \n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(120, activation='relu'),\n",
        "        tf.keras.layers.Dense(60, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        \n",
        "        tf.keras.layers.Dense(4, activation='softmax')\"\"\"\n",
        "   \n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(1300, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n",
        "cnn.add(tf.keras.layers.Conv2D(600, (3, 3), padding=\"same\", activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(600, (3, 3), activation='relu', padding=\"same\"))\n",
        "cnn.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(64, (5, 5), activation='relu', padding=\"same\"))\n",
        "cnn.add(tf.keras.layers.Conv2D(64, (5,5), padding=\"same\", activation='relu'))\n",
        "#cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(5, 5)))\n",
        "   \n",
        "cnn.add(tf.keras.layers.Conv2D(32, (5, 5), activation='relu', padding=\"same\"))\n",
        "cnn.add(tf.keras.layers.Conv2D(32, (5, 5), padding=\"same\", activation='relu'))\n",
        "\n",
        "\n",
        "#cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "#cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding=\"same\", activation='relu'))\n",
        "cnn.add(tf.keras.layers.BatchNormalization(renorm=True))\n",
        "\n",
        "\n",
        "\n",
        "cnn.add(Dense(16, activation='relu'))\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(activation='relu', units=256))\n",
        "cnn.add(tf.keras.layers.Dense(activation='relu', units=128))\n",
        "cnn.add(tf.keras.layers.Dropout(0.4, seed=seed_value))\n",
        "cnn.add(tf.keras.layers.Dense(activation='relu', units=64))\n",
        "cnn.add(tf.keras.layers.Dense(activation='relu', units=16))\n",
        "cnn.add(tf.keras.layers.Dense(activation='softmax', units=4))\n",
        "\n",
        "\n",
        "'''for layer in cnn.layers:\n",
        "  layer.trainable=True\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=.01),loss='categorical_crossentropy',metrics=[AUC()])\n",
        "# categorical cross entropy is taken since its used as a loss function for\n",
        "# multi-class classification problems where there are two or more output labels.\n",
        "# using Adam optimizer for better performance\n",
        "# other optimizers such as sgd can also be used depending upon the model\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping( patience=10,\n",
        "                                          min_delta=0.001,\n",
        "                                          restore_best_weights=True)\n",
        "checkpoint=checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"/content/gdrive/MyDrive/Dataset/output/checkpoints/my_modelfra2_{epoch}.h5\",\n",
        "   save_best_only=True,\n",
        "    verbose=1\n",
        "\n",
        ")\n",
        "reduce1=tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,  # reduce the learning rate by a factor of 0.1\n",
        "    patience=10,  # after 10 epochs with no improvement\n",
        "    verbose=1)\n",
        "callbacks=[early,checkpoint,reduce1]\n",
        "\n",
        "# early stopping call back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxhMnWqg6DKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0159e46f-48f0-433e-ff6f-b091fa2b56e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 8, 8, 1300)        23962900  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 600)         7020600   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 600)         3240600   \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 128)         691328    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 4, 4, 128)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 64)          204864    \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 64)          102464    \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 4, 4, 32)          51232     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 4, 4, 32)          25632     \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 32)          9248      \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 4, 4, 24)          6936      \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 16)          3472      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 4, 4, 16)         112       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4, 4, 16)          272       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59,163,008\n",
            "Trainable params: 35,575,216\n",
            "Non-trainable params: 23,587,792\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIBMjP0Ujy2I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1abb97f9-e8b6-41f7-ac23-d30cbeaa0cbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from sklearn.utils import class_weight\\n\\ndef get_weight(t):\\n    class_weight_current =  class_weight.compute_class_weight('balanced', classes=np.unique(t), y=t)\\n    return class_weight_current\\n\\n    \\ntrain_classes = train_generator.classes\\n\\nclass_weight =  get_weight(train_classes )\\nclass_weight = dict(zip(np.unique(train_classes), class_weight))\\nclass_weight\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "\"\"\"from sklearn.utils import class_weight\n",
        "\n",
        "def get_weight(t):\n",
        "    class_weight_current =  class_weight.compute_class_weight('balanced', classes=np.unique(t), y=t)\n",
        "    return class_weight_current\n",
        "\n",
        "    \n",
        "train_classes = train_generator.classes\n",
        "\n",
        "class_weight =  get_weight(train_classes )\n",
        "class_weight = dict(zip(np.unique(train_classes), class_weight))\n",
        "class_weight\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn = tf.keras.models.load_model('/content/gdrive/MyDrive/Dataset/output/checkpoints/my_modelfra_1.h5')"
      ],
      "metadata": {
        "id": "fFqZLChPD9_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QSWgf6P8xVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "e02c0f31-7a8c-40e3-a818-534502970072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start Model Trainning Timestamp: 2023-01-09 20:11:43\n",
            "Epoch 1/100\n",
            "25/54 [============>.................] - ETA: 17:29 - loss: 9128.6768 - auc: 0.5236"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-7e1227792a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start Model Trainning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m history = cnn.fit(train_generator,\n\u001b[0m\u001b[1;32m      9\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "epochs=100\n",
        "\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = val_generator.n//val_generator.batch_size\n",
        "\n",
        "print(\"start Model Trainning\", date_time(1))\n",
        "# fit model\n",
        "history = cnn.fit(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                    )\n",
        "\n",
        "\n",
        "print(\"Completed Model Trainning\", date_time(1))\n",
        "\n",
        "#create_charts(cnn, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUe5gMMmCN_D"
      },
      "outputs": [],
      "source": [
        "cnn.save(\"/content/gdrive/MyDrive/Dataset/output/models/Model_frame.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0gK9kfKk0qu"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_charts(cnn, cnn_model):\n",
        "    ## Define train & validation loss\n",
        "    train_loss = cnn_model.history['loss']\n",
        "    val_loss = cnn_model.history['val_loss']\n",
        "    \n",
        "    ## Define train & validation AUC\n",
        "    train_auc_name = list(cnn_model.history.keys())[3]\n",
        "    val_auc_name = list(cnn_model.history.keys())[1]\n",
        "    train_auc = cnn_model.history[train_auc_name]\n",
        "    val_auc = cnn_model.history[val_auc_name]\n",
        "    \n",
        "    ## Define y_pred & y_true\n",
        "    y_true = test_generator.classes\n",
        "    Y_pred = cnn.predict(test_generator, steps = len(test_generator))\n",
        "    y_pred = (Y_pred > 0.5).T[0]\n",
        "    y_pred_prob = Y_pred.T[0]\n",
        "    \n",
        "    ## PLOT ##\n",
        "    fig = plt.figure(figsize=(13, 10))\n",
        "    \n",
        "    ## PLOT 1: TRAIN VS. VALIDATION LOSS \n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title(\"Training vs. Validation Loss\")\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(val_loss, label='validation loss')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "\n",
        "    ## PLOT 2: TRAIN VS. VALIDATION AUC\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title(\"Training vs. Validation AUC Score\")\n",
        "    plt.plot(train_auc, label='training auc')\n",
        "    plt.plot(val_auc, label='validation auc')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "    \n",
        "    ## PLOT 3: CONFUSION MATRIX\n",
        "    plt.subplot(2,2,3)\n",
        "      # Set up the labels for in the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    names = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
        "    counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    ticklabels = ['Normal', 'Pneumonia']\n",
        "\n",
        "      # Create confusion matrix as heatmap\n",
        "    sns.set(font_scale = 1.4)\n",
        "    ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Oranges', xticklabels=ticklabels, yticklabels=ticklabels )\n",
        "    plt.xticks(size=12)\n",
        "    plt.yticks(size=12)\n",
        "    plt.title(\"Confusion Matrix\") #plt.title(\"Confusion Matrix\\n\", fontsize=10)\n",
        "    plt.xlabel(\"Predicted\", size=14)\n",
        "    plt.ylabel(\"Actual\", size=14) \n",
        "    #plt.savefig('cm.png', transparent=True) \n",
        "    \n",
        "    ## PLOT 4: ROC CURVE\n",
        "    plt.subplot(2,2,4)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    plt.title('ROC Curve')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label = \"Random (AUC = 50%)\")\n",
        "    plt.plot(fpr, tpr, label='CNN (AUC = {:.2f}%)'.format(auc*100))\n",
        "    plt.xlabel('False Positive Rate', size=14)\n",
        "    plt.ylabel('True Positive Rate', size=14)\n",
        "    plt.legend(loc='best')\n",
        "    #plt.savefig('roc.png', bbox_inches='tight', pad_inches=1)\n",
        "    \n",
        "    ## END PLOTS\n",
        "    plt.tight_layout();\n",
        "    \n",
        "    ## Summary Statistics\n",
        "    TN, FP, FN, TP = cm.ravel() # cm[0,0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "    accuracy = (TP + TN) / np.sum(cm) # % positive out of all predicted positives\n",
        "    precision = TP / (TP+FP) # % positive out of all predicted positives\n",
        "    recall =  TP / (TP+FN) # % positive out of all supposed to be positives\n",
        "    specificity = TN / (TN+FP) # % negative out of all supposed to be negatives\n",
        "    f1 = 2*precision*recall / (precision + recall)\n",
        "    stats_summary = '[Summary Statistics]\\nAccuracy = {:.2%} | Precision = {:.2%} | Recall = {:.2%} | Specificity = {:.2%} | F1 Score = {:.2%}'.format(accuracy, precision, recall, specificity, f1)\n",
        "    print(stats_summary)"
      ],
      "metadata": {
        "id": "WLDDya9dags-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_charts(cnn,history)"
      ],
      "metadata": {
        "id": "EBX-Y3xqahb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGdxcoqPCV9u"
      },
      "outputs": [],
      "source": [
        "# store results\n",
        "acc = history.history['auc']\n",
        "val_acc = history.history['val_auc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        " \n",
        "# plot results\n",
        "# accuracy\n",
        "plt.figure(figsize=(10, 16))\n",
        "plt.rcParams['figure.figsize'] = [16, 9]\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'\\nTraining and Validation Accuracy. \\nTrain Accuracy:{str(acc[-1])}\\nValidation Accuracy: {str(val_acc[-1])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lingEFbGJFez"
      },
      "outputs": [],
      "source": [
        "# loss\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title(f'Training and Validation Loss. \\nTrain Loss:{str(loss[-1])}\\nValidation Loss: {str(val_loss[-1])}')\n",
        "plt.xlabel('epoch')\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GORNvGrvJaH-"
      },
      "outputs": [],
      "source": [
        "accuracy_score = cnn.evaluate(val_generator)\n",
        "print(accuracy_score)\n",
        "print(\"Accuracy: {:.4f}%\".format(accuracy_score[1] * 100))\n",
        " \n",
        "print(\"Loss: \",accuracy_score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOL1FjKJyS3"
      },
      "outputs": [],
      "source": [
        "test_img_path = \"/content/gdrive/MyDrive/Dataset/data2/test/score0/convex_1048_1126_8.jpg\"\n",
        " \n",
        "img = cv2.imread(test_img_path)\n",
        "resized_img = cv2.resize(img, (331, 331)).reshape(-1, 331, 331, 3)/255\n",
        " \n",
        "plt.figure(figsize=(6,6))\n",
        "plt.title(\"TEST IMAGE\")\n",
        "plt.imshow(resized_img[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeT102GqKHhp"
      },
      "outputs": [],
      "source": [
        "'''predictions = []\n",
        "images=[]\n",
        "for root, dirs, files in os.walk(test_path, topdown=False):\n",
        "   for image in files:\n",
        "      image=os.path.join(root, image)\n",
        "      images.append(image)\n",
        "      img = tf.keras.preprocessing.image.load_img(image)\n",
        "      img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "      img = tf.keras.preprocessing.image.smart_resize(img, (688, 1760))\n",
        "      img = tf.reshape(img, (-1, 688, 1760, 3))\n",
        "      prediction = model.predict(img/255)\n",
        "      predictions.append(np.argmax(prediction))\n",
        "sample=pd.DataFrame(images)  \n",
        "\n",
        "my_submission = pd.DataFrame({'image_id': images, 'label':  predictions})\n",
        "if os.path.exists('/content/gdrive/MyDrive/Dataset/output/submission.csv'):\n",
        "  os.remove('/content/gdrive/MyDrive/Dataset/output/submission.csv')\n",
        "my_submission.to_csv('/content/gdrive/MyDrive/Dataset/output/submission.csv', index=False)\n",
        " \n",
        "# Submission file ouput\n",
        "print(\"Submission File: \\n---------------\\n\")\n",
        "print(my_submission.head()) # Displaying first five predicted output'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "180a8cf1fc48fa92a53c2d4bd1ff24d9f8e77dd7507839b4c169d20814ecdb3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}