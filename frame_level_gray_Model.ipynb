{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiheddachraoui/Covid19-ultrasound-video-scoring/blob/main/frame_level_gray_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfuqyGdp0cM7"
      },
      "source": [
        "# scoring using Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exCuSrEN0cM7"
      },
      "source": [
        "## Import The Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IU3as4W0cM7",
        "outputId": "4d5c1d19-23d5-435d-cd2d-cd1f547168b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# basics\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "import cv2\n",
        "import pickle \n",
        "\n",
        "import shutil\n",
        "# Keras Libraries\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer, Activation\n",
        "from keras.utils import  load_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.metrics import AUC\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import keras.backend\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from tensorflow.python.framework import ops\n",
        "import inspect\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-txyLjSbeifK"
      },
      "source": [
        "## utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI2-jctMeP0-"
      },
      "outputs": [],
      "source": [
        "# print date and time for given type of representation\n",
        "def date_time(x):\n",
        "    if x==1:\n",
        "        return 'Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
        "    if x==2:    \n",
        "        return 'Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now())\n",
        "    if x==3:  \n",
        "        return 'Date now: %s' % datetime.datetime.now()\n",
        "    if x==4:  \n",
        "        return 'Date today: %s' % datetime.date.today() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eshN2RCXepJg"
      },
      "outputs": [],
      "source": [
        "# reset tensorflow graph tp free up memory and resource allocation \n",
        "def reset_graph(model=None):\n",
        "    if model:\n",
        "        try:\n",
        "            del model\n",
        "        except:\n",
        "            return False\n",
        "    \n",
        "   \n",
        "    ops.reset_default_graph()\n",
        "    K.clear_session()\n",
        "    \n",
        "    gc.collect()\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "# reset callbacks \n",
        "def reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
        "    checkpoint = None\n",
        "    reduce_lr = None\n",
        "    early_stopping = None\n",
        "    tensorboard = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMObDy_hqRZ"
      },
      "outputs": [],
      "source": [
        "def create_directory(directory_path, remove=False):\n",
        "    if remove and os.path.exists(directory_path):\n",
        "        try:\n",
        "            shutil.rmtree(directory_path)\n",
        "            os.mkdir(directory_path)\n",
        "        except:\n",
        "            print(\"Could not remove directory : \", directory_path)\n",
        "            return False\n",
        "    else:\n",
        "        try:\n",
        "            os.mkdir(directory_path)\n",
        "        except:\n",
        "            print(\"Could not create directory: \", directory_path)\n",
        "            return False\n",
        "        \n",
        "    return True\n",
        "\n",
        "# Removes directory, if directory exists \n",
        "def remove_directory(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        try:\n",
        "            shutil.rmtree(directory_path)\n",
        "        except:\n",
        "            print(\"Could not remove directory : \", directory_path)\n",
        "            return False\n",
        "        \n",
        "    return True\n",
        "\n",
        "def clear_directory(directory_path):\n",
        "    dirs_files = os.listdir(directory_path)\n",
        "    \n",
        "    for item in dirs_files:\n",
        "#         item_path = os.path.join(directory_path, item)\n",
        "        item_path = directory_path+ item\n",
        "        \n",
        "        try:\n",
        "            if os.path.isfile(item_path):\n",
        "                os.unlink(item_path)\n",
        "            elif os.path.isdir(item_path): \n",
        "                shutil.rmtree(item_path)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            \n",
        "    return True\n",
        "\n",
        "\n",
        "def remove_empty_folders(path, removeRoot=True):\n",
        "    if not os.path.isdir(path):\n",
        "        return\n",
        "    \n",
        "    # remove empty subfolders\n",
        "    files = os.listdir(path)\n",
        "    \n",
        "    if len(files):\n",
        "        for f in files:\n",
        "            fullpath = os.path.join(path, f)\n",
        "            \n",
        "            if os.path.isdir(fullpath):\n",
        "                remove_empty_folders(fullpath)\n",
        "\n",
        "    # if folder empty, delete it\n",
        "    files = os.listdir(path)\n",
        "    \n",
        "    if len(files) == 0 and removeRoot:\n",
        "        print(\"Removing empty folder:\", path)\n",
        "        os.rmdir(path)\n",
        "        \n",
        "        \n",
        "def dir_file_count(directory):\n",
        "    return sum([len(files) for r, d, files in os.walk(directory)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmSqutyd-nCA"
      },
      "outputs": [],
      "source": [
        "def create_charts(cnn, cnn_model):\n",
        "    ## Define train & validation loss\n",
        "    train_loss = cnn_model.history['loss']\n",
        "    val_loss = cnn_model.history['val_loss']\n",
        "    \n",
        "    ## Define train & validation AUC\n",
        "    train_auc_name = list(cnn_model.history.keys())[3]\n",
        "    val_auc_name = list(cnn_model.history.keys())[1]\n",
        "    train_auc = cnn_model.history[train_auc_name]\n",
        "    val_auc = cnn_model.history[val_auc_name]\n",
        "    \n",
        "    ## Define y_pred & y_true\n",
        "    y_true = test_generator.classes\n",
        "    Y_pred = cnn.predict_generator(test_generator, steps = len(test_generator))\n",
        "    y_pred = (Y_pred > 0.5).T[0]\n",
        "    y_pred_prob = Y_pred.T[0]\n",
        "    \n",
        "    ## PLOT ##\n",
        "    fig = plt.figure(figsize=(13, 10))\n",
        "    \n",
        "    ## PLOT 1: TRAIN VS. VALIDATION LOSS \n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title(\"Training vs. Validation Loss\")\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(val_loss, label='validation loss')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "\n",
        "    ## PLOT 2: TRAIN VS. VALIDATION AUC\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title(\"Training vs. Validation AUC Score\")\n",
        "    plt.plot(train_auc, label='training auc')\n",
        "    plt.plot(val_auc, label='validation auc')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "    \n",
        "    ## PLOT 3: CONFUSION MATRIX\n",
        "    plt.subplot(2,2,3)\n",
        "      # Set up the labels for in the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    names = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
        "    counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    ticklabels = ['Normal', 'Pneumonia']\n",
        "\n",
        "      # Create confusion matrix as heatmap\n",
        "    sns.set(font_scale = 1.4)\n",
        "    ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Oranges', xticklabels=ticklabels, yticklabels=ticklabels )\n",
        "    plt.xticks(size=12)\n",
        "    plt.yticks(size=12)\n",
        "    plt.title(\"Confusion Matrix\") #plt.title(\"Confusion Matrix\\n\", fontsize=10)\n",
        "    plt.xlabel(\"Predicted\", size=14)\n",
        "    plt.ylabel(\"Actual\", size=14) \n",
        "    #plt.savefig('cm.png', transparent=True) \n",
        "    \n",
        "    ## PLOT 4: ROC CURVE\n",
        "    plt.subplot(2,2,4)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    plt.title('ROC Curve')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label = \"Random (AUC = 50%)\")\n",
        "    plt.plot(fpr, tpr, label='CNN (AUC = {:.2f}%)'.format(auc*100))\n",
        "    plt.xlabel('False Positive Rate', size=14)\n",
        "    plt.ylabel('True Positive Rate', size=14)\n",
        "    plt.legend(loc='best')\n",
        "    #plt.savefig('roc.png', bbox_inches='tight', pad_inches=1)\n",
        "    \n",
        "    ## END PLOTS\n",
        "    plt.tight_layout();\n",
        "    \n",
        "    ## Summary Statistics\n",
        "    TN, FP, FN, TP = cm.ravel() # cm[0,0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "    accuracy = (TP + TN) / np.sum(cm) # % positive out of all predicted positives\n",
        "    precision = TP / (TP+FP) # % positive out of all predicted positives\n",
        "    recall =  TP / (TP+FN) # % positive out of all supposed to be positives\n",
        "    specificity = TN / (TN+FP) # % negative out of all supposed to be negatives\n",
        "    f1 = 2*precision*recall / (precision + recall)\n",
        "    stats_summary = '[Summary Statistics]\\nAccuracy = {:.2%} | Precision = {:.2%} | Recall = {:.2%} | Specificity = {:.2%} | F1 Score = {:.2%}'.format(accuracy, precision, recall, specificity, f1)\n",
        "    print(stats_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrRD9ORterTQ"
      },
      "source": [
        "## code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "044vnlVP0cM8",
        "outputId": "d2c6dd45-d9ce-411b-be78-8e1898013f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing empty folder: /content/gdrive/MyDrive/Dataset/output/models/2023-01-05 01-28-26\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "input_directory = \"/content/gdrive/MyDrive/Dataset/grayscale_data/\"\n",
        "output_directory = \"/content/gdrive/MyDrive/Dataset/output/\"\n",
        "\n",
        "train_path = input_directory + r\"train\"\n",
        "val_path = input_directory + r\"valid\"\n",
        "test_path = input_directory + r\"test\"\n",
        "\n",
        "figure_directory = \"output/figures\" \n",
        "file_name_pred_batch = figure_directory+r\"/result\"\n",
        "file_name_pred_sample = figure_directory+r\"/sample\"\n",
        "\n",
        "main_model_dir = output_directory + r\"models/\"\n",
        "main_log_dir = output_directory + r\"logs/\"\n",
        "\n",
        "\n",
        "clear_directory(main_log_dir)\n",
        "remove_empty_folders(main_model_dir, False)\n",
        "\n",
        "\n",
        "model_dir = main_model_dir + time.strftime('%Y-%m-%d %H-%M-%S') + \"/\"\n",
        "log_dir = main_log_dir + time.strftime('%Y-%m-%d %H-%M-%S')\n",
        "\n",
        "create_directory(model_dir, remove=True)\n",
        "create_directory(log_dir, remove=True)\n",
        "\n",
        "model_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9I5nUSRexqB"
      },
      "outputs": [],
      "source": [
        "reset_graph()\n",
        "reset_callbacks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTu9IeSrhOzl"
      },
      "source": [
        "### callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6EluX-fYKo",
        "outputId": "1c56b896-fc4e-4653-da6d-2ced011b51d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settting Callbacks at  Timestamp: 2023-01-05 01:30:09\n",
            "Set Callbacks at  Timestamp: 2023-01-05 01:30:09\n"
          ]
        }
      ],
      "source": [
        "print(\"Settting Callbacks at \", date_time(1))\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_file, \n",
        "    monitor='val_acc', \n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    batch_size=batch_size,\n",
        "    update_freq = 'batch')\n",
        "\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    cooldown=2,\n",
        "    min_lr=0.0000000001,\n",
        "    verbose=1)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
        "# callbacks = [checkpoint, tensorboard]\n",
        "#-----------------------------------------------------------------------------------------------------------------#\n",
        "print(\"Set Callbacks at \", date_time(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzhneEFRlzUM"
      },
      "source": [
        "### preprosessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMFmruAz0cM7"
      },
      "outputs": [],
      "source": [
        "# Set a seed value\n",
        "seed_value= 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "#tf.set_random_seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
        "#model.add(Dropout(0.25, seed=seed_value))\n",
        "\n",
        "#6 Configure a new global `tensorflow` session\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUJRKGr70cM8"
      },
      "outputs": [],
      "source": [
        "#hyper_mode = 'grayscale'\n",
        "#imagesize=(688, 1760, 3)\n",
        "#imagesize=(688, 880,6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/gdrive/MyDrive/Dataset/grayscale_data/test/score0/convex_1048_1124_10_frame128_0.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "  # Resize the image to the desired input size\n",
        "  #resized_image = cv2.resize(image, (224, 224))\n",
        "\n",
        "  # Normalize the pixel values\n",
        "  #normalized_image = resized_image / 127.5 - 1\n",
        "\n",
        "  # Convert the image to a 4D tensor\n",
        "input_tensor = tf.expand_dims(image, 0)\n",
        "input_tensor.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvjXBowka8pH",
        "outputId": "47167016-3d67-4413-8744-cb891cd75e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 688, 880, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEmQB-Lg0cM8",
        "outputId": "d09eb6ba-61c0-48f7-cd89-bca417a26b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5840 images belonging to 4 classes.\n",
            "Found 825 images belonging to 4 classes.\n",
            "Found 1073 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0, \n",
        "                                   shear_range = 0.02,\n",
        "                                   zoom_range = 0.02, \n",
        "                                 \n",
        "                                   #preprocessing_function=preprocess_image ,\n",
        "                                   horizontal_flip = False)\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                                              \n",
        "                                                               #preprocessing_function=preprocess_image \n",
        "                                                              ) \n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                                                \n",
        "                                                               #preprocessing_function=preprocess_image\n",
        "                                                               ) \n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory = train_path, \n",
        "                                                    \n",
        "                                                    target_size = (330, 330), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "val_generator = val_datagen.flow_from_directory(directory = val_path, \n",
        "                                                 \n",
        "                                                    target_size = (330, 330), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "test_generator = test_datagen.flow_from_directory(directory = test_path, \n",
        "                                                 \n",
        "                                                    target_size = (330, 330), # image height , image width\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    #color_mode='grayscale',\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=42)\n",
        "\n",
        "test_generator.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgfidq8346qn",
        "outputId": "c562c1bc-952a-4791-90b5-baaf69098d57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 330, 330, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x,y = next(train_generator)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OX8NyzKs5Zlo",
        "outputId": "a4b35e85-ae8b-487f-882f-239803659efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"a = train_generator.class_indices\\nclass_names = list(a.keys())  # storing class/breed names in a list\\n \\n \\ndef plot_images(img, labels):\\n    plt.figure(figsize=[15, 10])\\n    for i in range(batch_size):\\n        plt.subplot(5, 5, i+1)\\n        plt.imshow(img[i])\\n        plt.title(class_names[np.argmax(labels[i])])\\n        plt.axis('off')\\n \\nplot_images(x,y)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "'''a = train_generator.class_indices\n",
        "class_names = list(a.keys())  # storing class/breed names in a list\n",
        " \n",
        " \n",
        "def plot_images(img, labels):\n",
        "    plt.figure(figsize=[15, 10])\n",
        "    for i in range(batch_size):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(img[i])\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis('off')\n",
        " \n",
        "plot_images(x,y)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgR0YAgrktB1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxNPT3951lM"
      },
      "outputs": [],
      "source": [
        "# load the InceptionResNetV2 architecture with imagenet weights as base\n",
        "base_model = tf.keras.applications.resnet50.ResNet50(\n",
        "                     include_top=False,\n",
        "                     weights='imagenet',\n",
        "                     input_shape=(330,330,3),\n",
        "                     \n",
        "                     \n",
        "                     )\n",
        " \n",
        "base_model.trainable=False\n",
        "\n",
        "# For freezing the layer we make use of layer.trainable = False\n",
        "# means that its internal state will not change during training.\n",
        "# model's trainable weights will not be updated during fit(),\n",
        "# and also its state updates will not run.\n",
        " \n",
        "model = tf.keras.Sequential([\n",
        "        base_model,  \n",
        "        \n",
        "        tf.keras.layers.BatchNormalization(renorm=True),\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1000, activation='relu'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        \n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(120, activation='relu'),\n",
        "        tf.keras.layers.Dense(60, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        \n",
        "        tf.keras.layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "'''for layer in model.layers:\n",
        "        layer.trainable=True'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# categorical cross entropy is taken since its used as a loss function for\n",
        "# multi-class classification problems where there are two or more output labels.\n",
        "# using Adam optimizer for better performance\n",
        "# other optimizers such as sgd can also be used depending upon the model\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping( patience=10,\n",
        "                                          min_delta=0.001,\n",
        "                                          restore_best_weights=True)\n",
        "checkpoint=checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"/content/gdrive/MyDrive/Dataset/output/models\",\n",
        "   save_freq=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks=[early,checkpoint]\n",
        "# early stopping call back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxhMnWqg6DKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea536dd-8e53-4f74-b50f-e3833459024a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 11, 11, 2048)      23587712  \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 11, 11, 2048)     14336     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              2049000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               512512    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 120)               15480     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 60)                7260      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 60)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4)                 244       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,350,768\n",
            "Trainable params: 2,752,816\n",
            "Non-trainable params: 23,597,952\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIBMjP0Ujy2I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7d9dffac-a83d-45f7-c37b-7981cce80066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from sklearn.utils import class_weight\\n\\ndef get_weight(t):\\n    class_weight_current =  class_weight.compute_class_weight('balanced', classes=np.unique(t), y=t)\\n    return class_weight_current\\n\\n    \\ntrain_classes = train_generator.classes\\n\\nclass_weight =  get_weight(train_classes )\\nclass_weight = dict(zip(np.unique(train_classes), class_weight))\\nclass_weight\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "\"\"\"from sklearn.utils import class_weight\n",
        "\n",
        "def get_weight(t):\n",
        "    class_weight_current =  class_weight.compute_class_weight('balanced', classes=np.unique(t), y=t)\n",
        "    return class_weight_current\n",
        "\n",
        "    \n",
        "train_classes = train_generator.classes\n",
        "\n",
        "class_weight =  get_weight(train_classes )\n",
        "class_weight = dict(zip(np.unique(train_classes), class_weight))\n",
        "class_weight\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QSWgf6P8xVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a02ffa9-4bd5-421b-ad9a-9f48e676028e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start Model Trainning Timestamp: 2023-01-05 01:30:16\n",
            "Epoch 1/100\n",
            " 9/45 [=====>........................] - ETA: 7:33 - loss: 1.4044 - accuracy: 0.2578\n",
            "Epoch 1: saving model to /content/gdrive/MyDrive/Dataset/output/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/45 [===========>..................] - ETA: 25:22 - loss: 1.3896 - accuracy: 0.2819\n",
            "Epoch 1: saving model to /content/gdrive/MyDrive/Dataset/output/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/45 [==================>...........] - ETA: 18:39 - loss: 1.3796 - accuracy: 0.2858\n",
            "Epoch 1: saving model to /content/gdrive/MyDrive/Dataset/output/models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/45 [=======================>......] - ETA: 9:53 - loss: 1.3748 - accuracy: 0.2880 "
          ]
        }
      ],
      "source": [
        "\n",
        "epochs=100\n",
        "\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = val_generator.n//val_generator.batch_size\n",
        "\n",
        "print(\"start Model Trainning\", date_time(1))\n",
        "# fit model\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callbacks,\n",
        "                    \n",
        "                    )\n",
        "\n",
        "\n",
        "print(\"Completed Model Trainning\", date_time(1))\n",
        "\n",
        "#create_charts(model, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUe5gMMmCN_D"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/gdrive/MyDrive/Dataset/output/models/Model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0gK9kfKk0qu"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGdxcoqPCV9u"
      },
      "outputs": [],
      "source": [
        "# store results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        " \n",
        "# plot results\n",
        "# accuracy\n",
        "plt.figure(figsize=(10, 16))\n",
        "plt.rcParams['figure.figsize'] = [16, 9]\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'\\nTraining and Validation Accuracy. \\nTrain Accuracy:{str(acc[-1])}\\nValidation Accuracy: {str(val_acc[-1])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lingEFbGJFez"
      },
      "outputs": [],
      "source": [
        "# loss\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title(f'Training and Validation Loss. \\nTrain Loss:{str(loss[-1])}\\nValidation Loss: {str(val_loss[-1])}')\n",
        "plt.xlabel('epoch')\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GORNvGrvJaH-"
      },
      "outputs": [],
      "source": [
        "accuracy_score = model.evaluate(val_generator)\n",
        "print(accuracy_score)\n",
        "print(\"Accuracy: {:.4f}%\".format(accuracy_score[1] * 100))\n",
        " \n",
        "print(\"Loss: \",accuracy_score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOL1FjKJyS3"
      },
      "outputs": [],
      "source": [
        "test_img_path = \"/content/gdrive/MyDrive/Dataset/data2/test/score0/convex_1048_1126_8.jpg\"\n",
        " \n",
        "img = cv2.imread(test_img_path)\n",
        "resized_img = cv2.resize(img, (331, 331)).reshape(-1, 331, 331, 3)/255\n",
        " \n",
        "plt.figure(figsize=(6,6))\n",
        "plt.title(\"TEST IMAGE\")\n",
        "plt.imshow(resized_img[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeT102GqKHhp"
      },
      "outputs": [],
      "source": [
        "'''predictions = []\n",
        "images=[]\n",
        "for root, dirs, files in os.walk(test_path, topdown=False):\n",
        "   for image in files:\n",
        "      image=os.path.join(root, image)\n",
        "      images.append(image)\n",
        "      img = tf.keras.preprocessing.image.load_img(image)\n",
        "      img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "      img = tf.keras.preprocessing.image.smart_resize(img, (688, 1760))\n",
        "      img = tf.reshape(img, (-1, 688, 1760, 3))\n",
        "      prediction = model.predict(img/255)\n",
        "      predictions.append(np.argmax(prediction))\n",
        "sample=pd.DataFrame(images)  \n",
        "\n",
        "my_submission = pd.DataFrame({'image_id': images, 'label':  predictions})\n",
        "if os.path.exists('/content/gdrive/MyDrive/Dataset/output/submission.csv'):\n",
        "  os.remove('/content/gdrive/MyDrive/Dataset/output/submission.csv')\n",
        "my_submission.to_csv('/content/gdrive/MyDrive/Dataset/output/submission.csv', index=False)\n",
        " \n",
        "# Submission file ouput\n",
        "print(\"Submission File: \\n---------------\\n\")\n",
        "print(my_submission.head()) # Displaying first five predicted output'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "180a8cf1fc48fa92a53c2d4bd1ff24d9f8e77dd7507839b4c169d20814ecdb3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}