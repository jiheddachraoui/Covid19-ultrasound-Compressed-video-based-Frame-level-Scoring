{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiheddachraoui/Covid19-ultrasound-video-scoring/blob/main/custom_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBkFwLNmwoqm"
      },
      "source": [
        "# Detecting Pneumonia using Convolutional Neural Network (CNN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q58crfelwoqp"
      },
      "source": [
        "## Import The Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diK6IXQ5woqp",
        "outputId": "30d87f48-6cbf-4fb2-dbd8-af1bf4650b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# basics\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "import pickle \n",
        "''''\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"'''\n",
        "\n",
        "# Keras Libraries\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, InputLayer, Activation\n",
        "from keras.utils import  load_img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.metrics import AUC\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import keras.backend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7TzoEES8woqr"
      },
      "outputs": [],
      "source": [
        "# Set a seed value\n",
        "seed_value= 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "#tf.set_random_seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "# 5. For layers that introduce randomness like dropout, make sure to set seed values \n",
        "#model.add(Dropout(0.25, seed=seed_value))\n",
        "\n",
        "#6 Configure a new global `tensorflow` session\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nGN3G40Bwoqr"
      },
      "outputs": [],
      "source": [
        "project_path = r\"/content/gdrive/MyDrive/Dataset/grayscale_data/\"\n",
        "train_path = project_path + r\"train\"\n",
        "test_path = project_path + r\"test\"\n",
        "val_path = project_path + r\"valid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jCHh0dCmwoqs"
      },
      "outputs": [],
      "source": [
        "hyper_dimension = 224\n",
        "hyper_batch_size = 128\n",
        "hyper_epochs = 100\n",
        "hyper_feature_maps=32\n",
        "\n",
        "## Training in grayscale instead of RGB\n",
        "hyper_channels = 1\n",
        "hyper_mode = 'grayscale'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MOHz2l-woqs",
        "outputId": "52f1d9de-4662-46a7-b133-70b42fffd64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5840 images belonging to 4 classes.\n",
            "Found 825 images belonging to 4 classes.\n",
            "Found 1073 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1.0/255.0, \n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2, \n",
        "                                   horizontal_flip = True)\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0) \n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory = train_path, \n",
        "                                                    target_size = (hyper_dimension, hyper_dimension),\n",
        "                                                    batch_size = hyper_batch_size, \n",
        "                                                    color_mode = hyper_mode,\n",
        "                                                    class_mode = 'categorical', \n",
        "                                                    seed = 42)\n",
        "val_generator = val_datagen.flow_from_directory(directory = val_path, \n",
        "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
        "                                                 batch_size = hyper_batch_size, \n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 color_mode = hyper_mode,\n",
        "                                                 shuffle=False,\n",
        "                                                 seed = 42)\n",
        "test_generator = test_datagen.flow_from_directory(directory = test_path, \n",
        "                                                 target_size = (hyper_dimension, hyper_dimension),\n",
        "                                                 batch_size = hyper_batch_size, \n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 color_mode = hyper_mode,\n",
        "                                                 shuffle=False,\n",
        "                                                 seed = 42)\n",
        "\n",
        "test_generator.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL1CoUkswoqt"
      },
      "source": [
        "## Top 3 Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGnBqg-Rwoqt"
      },
      "source": [
        "We first created a function to create two charts that shows the progress of training the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zMhdpj7zwoqu"
      },
      "outputs": [],
      "source": [
        "def create_charts(cnn, cnn_model):\n",
        "    ## Define train & validation loss\n",
        "    train_loss = cnn_model.history['loss']\n",
        "    val_loss = cnn_model.history['val_loss']\n",
        "    \n",
        "    ## Define train & validation AUC\n",
        "    train_auc_name = list(cnn_model.history.keys())[3]\n",
        "    val_auc_name = list(cnn_model.history.keys())[1]\n",
        "    train_auc = cnn_model.history[train_auc_name]\n",
        "    val_auc = cnn_model.history[val_auc_name]\n",
        "    \n",
        "    ## Define y_pred & y_true\n",
        "    y_true = test_generator.classes\n",
        "    Y_pred = cnn.predict(test_generator, steps = len(test_generator))\n",
        "    y_pred = (Y_pred > 0.5).T[0]\n",
        "    y_pred_prob = Y_pred.T[0]\n",
        "    \n",
        "    ## PLOT ##\n",
        "    fig = plt.figure(figsize=(13, 10))\n",
        "    \n",
        "    ## PLOT 1: TRAIN VS. VALIDATION LOSS \n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title(\"Training vs. Validation Loss\")\n",
        "    plt.plot(train_loss, label='training loss')\n",
        "    plt.plot(val_loss, label='validation loss')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "\n",
        "    ## PLOT 2: TRAIN VS. VALIDATION AUC\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title(\"Training vs. Validation AUC Score\")\n",
        "    plt.plot(train_auc, label='training auc')\n",
        "    plt.plot(val_auc, label='validation auc')\n",
        "    plt.xlabel(\"Number of Epochs\", size=14)\n",
        "    plt.legend()\n",
        "    \n",
        "    ## PLOT 3: CONFUSION MATRIX\n",
        "    plt.subplot(2,2,3)\n",
        "      # Set up the labels for in the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    names = ['True Negatives', 'False Positives', 'False Negatives', 'True Positives']\n",
        "    counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    ticklabels = ['Normal', 'Pneumonia']\n",
        "\n",
        "      # Create confusion matrix as heatmap\n",
        "    sns.set(font_scale = 1.4)\n",
        "    ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Oranges', xticklabels=ticklabels, yticklabels=ticklabels )\n",
        "    plt.xticks(size=12)\n",
        "    plt.yticks(size=12)\n",
        "    plt.title(\"Confusion Matrix\") #plt.title(\"Confusion Matrix\\n\", fontsize=10)\n",
        "    plt.xlabel(\"Predicted\", size=14)\n",
        "    plt.ylabel(\"Actual\", size=14) \n",
        "    #plt.savefig('cm.png', transparent=True) \n",
        "    \n",
        "    ## PLOT 4: ROC CURVE\n",
        "    plt.subplot(2,2,4)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    plt.title('ROC Curve')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label = \"Random (AUC = 50%)\")\n",
        "    plt.plot(fpr, tpr, label='CNN (AUC = {:.2f}%)'.format(auc*100))\n",
        "    plt.xlabel('False Positive Rate', size=14)\n",
        "    plt.ylabel('True Positive Rate', size=14)\n",
        "    plt.legend(loc='best')\n",
        "    #plt.savefig('roc.png', bbox_inches='tight', pad_inches=1)\n",
        "    \n",
        "    ## END PLOTS\n",
        "    plt.tight_layout();\n",
        "    \n",
        "    ## Summary Statistics\n",
        "    TN, FP, FN, TP = cm.ravel() # cm[0,0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
        "    accuracy = (TP + TN) / np.sum(cm) # % positive out of all predicted positives\n",
        "    precision = TP / (TP+FP) # % positive out of all predicted positives\n",
        "    recall =  TP / (TP+FN) # % positive out of all supposed to be positives\n",
        "    specificity = TN / (TN+FP) # % negative out of all supposed to be negatives\n",
        "    f1 = 2*precision*recall / (precision + recall)\n",
        "    stats_summary = '[Summary Statistics]\\nAccuracy = {:.2%} | Precision = {:.2%} | Recall = {:.2%} | Specificity = {:.2%} | F1 Score = {:.2%}'.format(accuracy, precision, recall, specificity, f1)\n",
        "    print(stats_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izUwYPMMwoqu"
      },
      "source": [
        "### Model #1: Conv2D x 3 + 64 dense units\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping( patience=10,\n",
        "                                          min_delta=0.001,\n",
        "                                          restore_best_weights=True)\n",
        "checkpoint=checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"/content/gdrive/MyDrive/Dataset/output/checkpoints/my_model_{epoch}.h5\",\n",
        "   save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks=[early,checkpoint]"
      ],
      "metadata": {
        "id": "5VZd6adBx03A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6H6BgF4woqv",
        "outputId": "bc56b1d7-913f-44ef-9486-77993ac424ea",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.2928 - auc: 0.6633 \n",
            "Epoch 1: val_loss improved from inf to 1.51573, saving model to /content/gdrive/MyDrive/Dataset/output/checkpoints/my_model_1.h5\n",
            "46/46 [==============================] - 1568s 34s/step - loss: 1.2928 - auc: 0.6633 - val_loss: 1.5157 - val_auc: 0.5253\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 1.1171 - auc: 0.7737\n",
            "Epoch 2: val_loss improved from 1.51573 to 1.49835, saving model to /content/gdrive/MyDrive/Dataset/output/checkpoints/my_model_2.h5\n",
            "46/46 [==============================] - 42s 904ms/step - loss: 1.1171 - auc: 0.7737 - val_loss: 1.4984 - val_auc: 0.5638\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.9141 - auc: 0.8569\n",
            "Epoch 3: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 900ms/step - loss: 0.9141 - auc: 0.8569 - val_loss: 1.8787 - val_auc: 0.5615\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.7183 - auc: 0.9137\n",
            "Epoch 4: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 894ms/step - loss: 0.7183 - auc: 0.9137 - val_loss: 2.1128 - val_auc: 0.5223\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.6014 - auc: 0.9398\n",
            "Epoch 5: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 44s 961ms/step - loss: 0.6014 - auc: 0.9398 - val_loss: 2.3438 - val_auc: 0.5801\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.5058 - auc: 0.9564\n",
            "Epoch 6: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 894ms/step - loss: 0.5058 - auc: 0.9564 - val_loss: 2.7911 - val_auc: 0.5247\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4588 - auc: 0.9637\n",
            "Epoch 7: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 895ms/step - loss: 0.4588 - auc: 0.9637 - val_loss: 2.3988 - val_auc: 0.5545\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.4144 - auc: 0.9701\n",
            "Epoch 8: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 895ms/step - loss: 0.4144 - auc: 0.9701 - val_loss: 2.8042 - val_auc: 0.5673\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3868 - auc: 0.9739\n",
            "Epoch 9: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 900ms/step - loss: 0.3868 - auc: 0.9739 - val_loss: 2.8469 - val_auc: 0.5228\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3684 - auc: 0.9756\n",
            "Epoch 10: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 896ms/step - loss: 0.3684 - auc: 0.9756 - val_loss: 2.6513 - val_auc: 0.5224\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3520 - auc: 0.9782\n",
            "Epoch 11: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 894ms/step - loss: 0.3520 - auc: 0.9782 - val_loss: 2.7336 - val_auc: 0.5142\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.3259 - auc: 0.9808\n",
            "Epoch 12: val_loss did not improve from 1.49835\n",
            "46/46 [==============================] - 41s 897ms/step - loss: 0.3259 - auc: 0.9808 - val_loss: 2.8732 - val_auc: 0.5679\n"
          ]
        }
      ],
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "                                \n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(activation='relu', units=64))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model1 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = len(val_generator), \n",
        "                              callbacks=callbacks,\n",
        "                              verbose=1)\n",
        "cnn1=cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9WUQDwQwoqv"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6PtUQSPwoqv"
      },
      "source": [
        "### Model #2: Conv2D x 4 + 64 dense units\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7BWYmnQGwoqv",
        "outputId": "b2040d3f-d980-4818-977c-e240b64f5788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/46 [======>.......................] - ETA: 15:44 - loss: 1.3607 - auc_2: 0.5890"
          ]
        }
      ],
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(activation='relu', units=64))\n",
        "cnn.add(Dropout(0.4))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model2 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              callbacks=callbacks,\n",
        "                              validation_steps = len(val_generator))\n",
        "cnn2=cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAyKWgq_woqw"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6nrsTM0woqw"
      },
      "source": [
        "### Model #3: Conv2D x 2 + 64 dense units\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "07nKe_-1woqw"
      },
      "outputs": [],
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(activation='relu', units=64))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model3 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              callbacks=callbacks,\n",
        "                              validation_steps = len(val_generator), \n",
        "                              verbose=2)\n",
        "cnn3=cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uqmsg1eIwoqw"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kU_FriGwoqx"
      },
      "source": [
        "### Model #4 (I): Conv2D x 3 + 128 dense units (100 epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WRs-lvNdwoqx"
      },
      "outputs": [],
      "source": [
        "# Conv2D x 3 + 128 dense units\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(activation='relu', units=128))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model4 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              callbacks=callbacks,\n",
        "                              validation_steps = len(val_generator), \n",
        "                              verbose=1)\n",
        "cnn4=cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVD7ltrrwoqx"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8QifrJRwoqx"
      },
      "source": [
        "### Model #4 (II): Conv2D x 3 + 128 dense units (200 epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hX6XAzE5woqx"
      },
      "outputs": [],
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(activation='relu', units=128))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model41 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              callbacks=callbacks,\n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = len(val_generator), \n",
        "                              verbose=2)\n",
        "cnn41=cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk3OI1k6woqy"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6XfvuoCwoqy"
      },
      "source": [
        "### Model #5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip0JeX18woqy"
      },
      "outputs": [],
      "source": [
        "## batch size: 64 Conv2D (filter: 32, 64, 128)  x 3 + different dense units + Batch Normalization\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=64, kernel_size=3))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Conv2D(filters=128, kernel_size=3))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(Dense(activation='relu', units=1024))\n",
        "cnn.add(Dense(activation='relu', units=512))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model5 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = len(val_generator),\n",
        "                              callbacks=callbacks, \n",
        "                              verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukcOHG4-woqy"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnunBKTDwoqy"
      },
      "source": [
        "### Model #6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Fg-3LqDwoqy"
      },
      "outputs": [],
      "source": [
        "## Conv2D (filter: 32, 64, 128)  x 3 + different dense units + Batch Normalization + Dropout Dense layer\n",
        "cnn = Sequential()\n",
        "cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=3, activation='elu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dropout(0.25))\n",
        "\n",
        "cnn.add(Conv2D(filters=64, kernel_size=3, activation='elu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dropout(0.3))\n",
        "\n",
        "cnn.add(Conv2D(filters=128, kernel_size=3, activation='elu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(Dropout(0.3))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(BatchNormalization())\n",
        "cnn.add(Dense(activation='relu', units=512))\n",
        "cnn.add(Dropout(0.25))\n",
        "cnn.add(Dense(activation='relu', units=256))\n",
        "cnn.add(Dropout(0.25))\n",
        "cnn.add(Dense(activation='softmax', units=4))\n",
        "\n",
        "cnn.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=[AUC()])\n",
        "cnn_model6 = cnn.fit(train_generator, \n",
        "                              steps_per_epoch = len(train_generator), \n",
        "                              epochs = hyper_epochs, \n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = len(val_generator), \n",
        "                              callbacks=callbacks,\n",
        "                              verbose=2)\n",
        "cnn6=cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPbyfN3Swoqz"
      },
      "outputs": [],
      "source": [
        "#create_charts(cnn, cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pmcLIBWwoqz"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "**Model #4 (200 epochs)** has the best result with the following architecture\n",
        "\n",
        "    cnn = Sequential()\n",
        "    cnn.add(InputLayer(input_shape=(hyper_dimension, hyper_dimension, hyper_channels)))\n",
        "\n",
        "    cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    cnn.add(Conv2D(filters=hyper_feature_maps, kernel_size=3, activation='relu'))\n",
        "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    cnn.add(Dense(activation='relu', units=128))\n",
        "    cnn.add(Dense(activation='sigmoid', units=1))\n",
        "\n",
        "    cnn.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=[AUC()])\n",
        "    cnn_model = cnn.fit(train_generator,\n",
        "                                  steps_per_epoch = len(train_generator),\n",
        "                                  epochs = 200,\n",
        "                                  validation_data = val_generator,\n",
        "                                  validation_steps = len(val_generator),\n",
        "                                  verbose=2)\n",
        "\n",
        "\n",
        "\n",
        "- AUC Score: 97.98%\n",
        "- Accuracy: 90.71%\n",
        "- Recall: 98.72% (FP = 8.49% and FN = 0.80%)\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "180a8cf1fc48fa92a53c2d4bd1ff24d9f8e77dd7507839b4c169d20814ecdb3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}